define flow metrics
flow
  use tremor::pipelines;

  define connector console from stdio
  with
    postprocessors = [ "separate" ],
    codec = "influx"
  end;

  # define the udp server
  define connector upd_in from udp_server
  with
    # define metrics interval
    metrics_interval_s = 5,
    # define the codec we use, in this case `influx` for the influx wire protocol
    codec = "influx",
    # define the preprocessors, we use separate to seperate events by lines
    preprocessors = ["separate"],
    # configure the connector itself, we listen to `0.0.0.0` on port `4242`
    config = {
      "url": "0.0.0.0:4242",
    }
  end;
  
  # define our http client
  define connector influx_out from tcp_client
  with
    # define metrics interval
    metrics_interval_s = 5,
    # we use the influx codec here as well
    codec = "influx",
    # define the postprocessors, we use separate to seperate events by lines
    postprocessors = ["separate"],
    # configure  the endpoint we're writing to    
    config = {
      "url": "questdb:9009",
      # We use a custom header to identify that we're tremor
    },
    reconnect = {
      "retry": {
        "interval_ms": 100,
        "growth_rate": 1,
      }
    }
  end;

  # Define our hasher
  define connector metrics from metrics;

  # define our metrics pipeline to batch metrics
  define pipeline metrics
  pipeline
    #!config metrics_interval_s = 5

    # use types for checking if values are a number
    use std::type;
    # use record for exploding keys
    use std::record;
    # use nanos for time
    use std::time::nanos;
    # use re for replace_all
    use std::re;

    # define a 10 second window
    define window `10secs` from tumbling
    with
      interval = nanos::from_seconds(10)
    end;

    # define a 1 minute window
    define window `1min` from tumbling
    with
      interval = nanos::from_minutes(1)
    end;


    # define our batch operator.
    # A batch will collect up to 3000 events, but never wait more then 5 seconds
    # before emitting.
    define operator batch from generic::batch
    with
      count = 3000,
      timeout = nanos::from_seconds(5)
    end;

    # create our batch operator
    create operator batch;

    # create our pre-aggregation stream
    create stream aggregate;

    # We change the structure of our event to be easier digestible for aggregation
    select {
        "measurement": re::replace_all("[ :\\./]", event.measurement, "_"), # QuestDB limitations
        "tags": event.tags,
        "field": group[2],
        "value": event.fields[group[2]],
        "timestamp": event.timestamp,
    }
    from in
    group by set(event.measurement, event.tags, each(record::keys(event.fields)))
    into aggregate
    having type::is_number(event.value);
 
    # create a stream for normalisation
    create stream normalize;

    select
    {
        "measurement": event.measurement,
        "tags": patch event.tags of insert "window" => window end,
        "stats": aggr::stats::hdr(event.value, [ "0.5", "0.9", "0.99", "0.999" ]),
        "field": event.field,
        "timestamp": aggr::win::first(event.timestamp), # we can't use min since it's a float
    }
    from aggregate[`10secs`, `1min`]
    group by set(event.measurement, event.tags, event.field)
    into normalize;

    select {
      "measurement":  event.measurement,
      "tags":  event.tags,
      "fields":  {
        "count_#{event.field}":  event.stats.count,
        "min_#{event.field}":  event.stats.min,
        "max_#{event.field}":  event.stats.max,
        "mean_#{event.field}":  event.stats.mean,
        "stdev_#{event.field}":  event.stats.stdev,
        "var_#{event.field}":  event.stats.var,
        "p50_#{event.field}":  event.stats.percentiles["0.5"],
        "p90_#{event.field}":  event.stats.percentiles["0.9"],
        "p99_#{event.field}":  event.stats.percentiles["0.99"],
        "p99_9_#{event.field}":  event.stats.percentiles["0.999"]
      },
      "timestamp": event.timestamp,
    }
    from normalize
    into batch;

    select event from batch into out;
  end;

  # Create the internal metrics collector
  create connector metrics;

  # Create the udp server
  create connector upd_in;

  # Create the http client
  create connector influx_out;

  # Create our pipeline
  create pipeline metrics from metrics;

  # Debug influx to stdio
  create connector console from console;
  create pipeline debug from pipelines::passthrough;

  # Connect the metrics to the pipeline
  connect /connector/metrics to /pipeline/metrics;
  # Connect the udp server to the pipeline
  connect /connector/upd_in to /pipeline/metrics;
  # Connect the pipeline to the inflix client
  connect /pipeline/metrics to /connector/influx_out;
  # connect /pipeline/metrics to /connector/console;
  connect /connector/metrics to /pipeline/debug;
  connect /pipeline/debug to /connector/console;

end;

# start our 
deploy flow metrics;
